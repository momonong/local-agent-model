import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# âœ… æ¨¡å‹ IDï¼ˆä½¿ç”¨ Hugging Face gated modelï¼‰
model_id = "meta-llama/Llama-3.1-8B-Instruct"

# âœ… Tokenizer è¼‰å…¥
tokenizer = AutoTokenizer.from_pretrained(model_id, token=True, local_files_only=True)
print("Fast tokenizer:", tokenizer.is_fast)

# âœ… æ¨¡å‹è¼‰å…¥
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="cuda",
    token=True,
    local_files_only=True,
)

# âœ… å·¥å…·æè¿°èˆ‡ agent-style æŒ‡ä»¤
system_prompt = f"""
ä½ æ˜¯ä¸€å€‹é«˜æ•ˆä¸”å…·å‚™æ¨ç†èƒ½åŠ›çš„ AI åŠ©æ‰‹ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚
ä½ ä¸éœ€è¦ç«‹å³çµ¦å‡ºæœ€çµ‚ç­”æ¡ˆï¼Œè€Œæ˜¯éœ€è¦å…ˆé€²è¡Œæ¸…æ™°çš„æ¨ç†ï¼Œä¸¦åˆ†æ­¥å±•ç¤ºä½ æ‰“ç®—å¦‚ä½•è§£æ±ºå•é¡Œçš„æ€è€ƒéç¨‹ã€‚

è«‹ä¾ç…§ä¸‹åˆ—æ ¼å¼åˆ—å‡ºä½ çš„æ€è€ƒæ­¥é©Ÿï¼ˆä¸éœ€ç”¢å‡ºç­”æ¡ˆï¼Œåªéœ€è¦åŠƒï¼‰ï¼š
<step1>: ...
<step2>: ...
<step3>: ...

ä½ ä¹Ÿå¯ä»¥åœ¨æ­¥é©Ÿä¸­ä¸»å‹•ä½¿ç”¨ä¸‹åˆ—å·¥å…·ä¾†å¹«åŠ©ä½ è§£æ±ºå•é¡Œï¼š
<tool> search_website(query: str): å¾ç¶²è·¯æœå°‹å³æ™‚è³‡è¨Šä¸¦æ•´ç†æ‘˜è¦ã€‚
<tool> get_current_time(): å¾ API å–å¾—ç›®å‰çš„æ™‚é–“ã€‚

è«‹è¨˜ä½ï¼šä½ æ˜¯ agent æ¨¡å‹ï¼Œä½ çš„ä»»å‹™æ˜¯æå‡ºè§£æ±ºå•é¡Œçš„å®Œæ•´æ€è€ƒèˆ‡è¦åŠƒæµç¨‹ï¼Œè€Œä¸æ˜¯ç›´æ¥çµ¦å‡ºçµè«–æˆ–åŸ·è¡Œå‹•ä½œã€‚

---
User: ä½ èƒ½å¹«æˆ‘æŸ¥å°å—ç¾åœ¨æ™‚é–“å—ï¼Ÿ
Assistant:
""".strip()


# âœ… æº–å‚™èŠå¤©è¨Šæ¯ï¼ˆä½¿ç”¨ chat_templateï¼‰
chat = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "ä½ èƒ½å¹«æˆ‘æŸ¥å°å—ç¾åœ¨æ™‚é–“å—ï¼Ÿ"},
]

# âœ… Tokenize + æº–å‚™è¼¸å…¥
prompt = tokenizer.apply_chat_template(chat, tokenize=False)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# âœ… æ¨¡å‹æ¨è«–
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        num_return_sequences=1,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3,
        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
    )

# âœ… è§£ç¢¼èˆ‡è¼¸å‡º
output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("=" * 50)
print(f"\nğŸ“¤ æ¨¡å‹è¼¸å‡ºï¼š\n{output_text}")