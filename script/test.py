import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# æ¨¡å‹è¨­å®š
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
adapter_path = "model/agent14b-lora"

# é‡åŒ–è¼‰å…¥è¨­å®šï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Tokenizer è¼‰å…¥
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=True,
    trust_remote_code=True,
)

# æ¨¡å‹ + LoRA adapter è¼‰å…¥
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_cfg,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval()

# âœ… æº–å‚™ promptï¼ˆä¸­è‹±æ–‡çš†å¯ï¼‰
chat = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€å€‹é«˜æ•ˆä¸”å…·å‚™æ¨ç†èƒ½åŠ›çš„ AI åŠ©æ‰‹ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
                   "ä½ ä¸éœ€è¦ç«‹å³çµ¦å‡ºæœ€çµ‚ç­”æ¡ˆï¼Œè€Œæ˜¯éœ€è¦å…ˆé€²è¡Œæ¸…æ™°çš„æ¨ç†ï¼Œ"
                   "ä¸¦åˆ†æ­¥å±•ç¤ºä½ æ‰“ç®—å¦‚ä½•è§£æ±ºå•é¡Œçš„æ€è€ƒéç¨‹ã€‚\n\n"
                   "è«‹ä¾ç…§ä¸‹åˆ—æ ¼å¼åˆ—å‡ºä½ çš„æ€è€ƒæ­¥é©Ÿï¼ˆä¸éœ€ç”¢å‡ºç­”æ¡ˆï¼Œåªéœ€è¦åŠƒï¼‰ï¼š\n"
                   "<step1>: ...\n<step2>: ...\n<step3>: ...\n\n"
                   "ä½ ä¹Ÿå¯ä»¥åœ¨æ­¥é©Ÿä¸­ä¸»å‹•ä½¿ç”¨ä¸‹åˆ—å·¥å…·ä¾†å¹«åŠ©ä½ è§£æ±ºå•é¡Œï¼š\n"
                   "<tool> search_website(query: str): å¾ç¶²è·¯æœå°‹å³æ™‚è³‡è¨Šä¸¦æ•´ç†æ‘˜è¦ã€‚\n"
                   "<tool> get_current_time(): å¾ API å–å¾—ç›®å‰çš„æ™‚é–“ã€‚\n\n"
                   "è«‹è¨˜ä½ï¼šä½ æ˜¯ agent æ¨¡å‹ï¼Œä½ çš„ä»»å‹™æ˜¯æå‡ºè§£æ±ºå•é¡Œçš„å®Œæ•´æ€è€ƒèˆ‡è¦åŠƒæµç¨‹ï¼Œè€Œä¸æ˜¯ç›´æ¥çµ¦å‡ºçµè«–æˆ–åŸ·è¡Œå‹•ä½œã€‚",
    },
    {"role": "user", "content": "ä½ èƒ½å¹«æˆ‘æŸ¥å°å—ç¾åœ¨æ™‚é–“å—ï¼Ÿ"}
]

# âœ… chat template è™•ç†æˆ prompt å­—ä¸²
prompt = tokenizer.apply_chat_template(chat, tokenize=False)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# âœ… æ¨¡å‹ç”Ÿæˆ
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=4096,
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        num_return_sequences=1,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3,
        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
    )

# âœ… è§£ç¢¼èˆ‡é¡¯ç¤º
output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("=" * 50)
print("\nğŸ“¤ æ¨¡å‹è¼¸å‡ºï¼š\n" + output_text)
