import time, gc, torch, psutil
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"

# BitsAndBytes é‡åŒ–è¨­å®šï¼š4-bit NF4
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
print("ğŸ”§ bnb config =", bnb_cfg)

# Tokenizer
t0 = time.time()
tok = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=True,
    trust_remote_code=True,   # å»ºè­°è£œä¸Š
    local_files_only=True,
)
print(f"ğŸš€ tokenizer loaded (fast={tok.is_fast}) in {time.time()-t0:.2f}s")

# é‡åŒ–æ¨¡å‹
t0 = time.time()
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_cfg,
    device_map="auto",
    trust_remote_code=True,
    # local_files_only=True,
)
load_sec = time.time() - t0
gpu_name = torch.cuda.get_device_name(model.device)
vram = torch.cuda.memory_allocated(model.device) / 1024**3
print(f"ğŸš€ model loaded to {gpu_name} in {load_sec:.2f}s, "
      f"VRAM now â‰ˆ {vram:.2f} GiB")

# Prompt
prompt = (
    "<|system|>\nä½ æ˜¯ä¸€å€‹å…·å‚™æ¨ç†èƒ½åŠ›ä¸”èƒ½åˆ†æ­¥è¦åŠƒçš„ AI åŠ©æ‰‹ã€‚\n"
    "<|user|>\nè«‹åˆ—å‡ºæŸ¥è©¢ã€Œå°å—ç¾åœ¨æ™‚é–“ã€çš„æ­¥é©Ÿï¼Œä¸è¦ç›´æ¥å›ç­”æ™‚é–“ã€‚\n<|assistant|>"
)
inputs = tok(prompt, return_tensors="pt").to(model.device)
print(f"âœï¸  prompt tokens = {inputs['input_ids'].shape[-1]}")

# ç”Ÿæˆ
t0 = time.time()
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        no_repeat_ngram_size=3,
        pad_token_id=tok.pad_token_id or tok.eos_token_id,
    )
gen_sec = time.time() - t0
out_tokens = outputs.shape[-1] - inputs["input_ids"].shape[-1]
print(f"ğŸ•’ generate() done in {gen_sec:.2f}s, new tokens = {out_tokens}")

# è§£ç¢¼ + é¡¯ç¤º
print("\nğŸ“¤ Model output â†“â†“â†“\n")
print(tok.decode(outputs[0], skip_special_tokens=True))

# é¡å¤–ï¼šé¡¯ç¤º Python/ç³»çµ±è¨˜æ†¶é«”
cpu_mem = psutil.Process().memory_info().rss / 1024**3
print(f"\nğŸ–¥ï¸  CPU RSS â‰ˆ {cpu_mem:.2f} GiB")
